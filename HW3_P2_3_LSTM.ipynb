{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hurricane195/Intro-to-Deep-Learning/blob/Homework_3/HW3_P2_3_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXJ7QOmqv1p9"
      },
      "source": [
        "Adjust the hyperparameters (fully connected network, number of hidden layers, and the number of hidden states) and compare your results (training and validation loss, computation complexity, model size, training and inference time, and the output sequence). Analyze their influence on accuracy, running time, and computational perplexity.\n",
        "\n",
        "**Increase the sequence length to 50. Perform the training and report the accuracy and model complexity results.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MC4eRxw1vpcG"
      },
      "outputs": [],
      "source": [
        "#Using a modided example of Dr. Tabkhi's \"RNN\" available at https://github.com/HamedTabkhi/Intro-to-DL/blob/main/RNN.py\n",
        "#Using a modided example of Dr. Tabkhi's \"RNN-CharDataset\" available at https://github.com/HamedTabkhi/Intro-to-DL/blob/main/RNN-CharDataset.py\n",
        "#Using a modided example of Dr. Tabkhi's \"shakespeare-loader.py\" available at https://github.com/HamedTabkhi/Intro-to-DL/blob/main/shakespeare-loader.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsIqPri1TA1l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "199KcmHMx1vo",
        "outputId": "4b85fec8-196c-4188-e698-94f1b2c13c5f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Check for CUDA support and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E-V7pjorX53"
      },
      "outputs": [],
      "source": [
        "#Download the dataset\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text  # This is the entire text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAxQQGCMsM4t"
      },
      "source": [
        "**MAXIMUM LENGTH OF INPUT SECQUENCES = 50**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ty3Ss4JVrdV_"
      },
      "outputs": [],
      "source": [
        "#Prepare the dataset\n",
        "sequence_length = 50\n",
        "# Create a character mapping to integers\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
        "int_to_char = {i: ch for i, ch in enumerate(chars)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIuvc0zPrhzj"
      },
      "outputs": [],
      "source": [
        "# Encode the text into integers\n",
        "encoded_text = [char_to_int[ch] for ch in text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4EWMpZRrkcR"
      },
      "outputs": [],
      "source": [
        "# Create sequences and targets\n",
        "sequences = []\n",
        "targets = []\n",
        "for i in range(0, len(encoded_text) - sequence_length):\n",
        "    seq = encoded_text[i:i+sequence_length]\n",
        "    target = encoded_text[i+sequence_length]\n",
        "    sequences.append(seq)\n",
        "    targets.append(target)\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "targets = np.array(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-updB7piHC6"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(sequences, targets, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofrCzO7QilqM"
      },
      "outputs": [],
      "source": [
        "# Converting data to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.long)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val = torch.tensor(X_val, dtype=torch.long)\n",
        "y_val = torch.tensor(y_val, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot47W5O6rpVt"
      },
      "outputs": [],
      "source": [
        "#Create a dataset class\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.sequences = sequences\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.sequences[index], self.targets[index]\n",
        "\n",
        "# Instantiate the dataset\n",
        "dataset = CharDataset(sequences, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaYFkmSkrwPN"
      },
      "outputs": [],
      "source": [
        "# Create datasets and data loaders\n",
        "batch_size = 64\n",
        "train_dataset = CharDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = CharDataset(X_val, y_val)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37fojvqSTe_7"
      },
      "outputs": [],
      "source": [
        "# Defining the RNN model\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        #This line takes the input tensor x, which contains indices of characters, and passes it through an embedding layer (self.embedding).\n",
        "        #The embedding layer converts these indices into dense vectors of fixed size.\n",
        "        #These vectors are learned during training and can capture semantic similarities between characters.\n",
        "        #The result is a higher-dimensional representation of the input sequence, where each character index is replaced by its corresponding embedding vector.\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.rnn = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        #The RNN layer returns two outputs:\n",
        "        #1- the output tensor containing the output of the RNN at each time step for each sequence in the batch,\n",
        "        #2-the hidden state (_) of the last time step (which is not used in this line, hence the underscore).\n",
        "        output, _ = self.rnn(embedded)\n",
        "        #The RNN's output contains the outputs for every time step,\n",
        "        #but for this task, we're only interested in the output of the last time step because we're predicting the next character after the sequence.\n",
        "        #output[:, -1, :] selects the last time step's output for every sequence in the batch (-1 indexes the last item in Python).\n",
        "        output = self.fc(output[:, -1, :])  # Get the output of the last RNN cell\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td4AGgC2TiHl"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "hidden_size = 50\n",
        "learning_rate = 0.1\n",
        "epochs = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08mM2r-QTkFT"
      },
      "outputs": [],
      "source": [
        "# Model, loss, and optimizer\n",
        "model = CharRNN(len(chars), hidden_size, len(chars)+10) # Increase the number of output neurons by 10\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3IEg0wRqCC1",
        "outputId": "03c07e50-0543-408f-a8a0-7063a0f126b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27475"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "#count trainable parameters of the model\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqpBZgkwUXuf",
        "outputId": "76b8b3b6-5920-4fc4-d99c-51501d734ad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25, Training Loss: 2.7960, Training Accuracy: 0.2559, Validation Loss: 2.7695, Validation Accuracy: 0.2737\n",
            "Epoch 2/25, Training Loss: 2.7812, Training Accuracy: 0.2613, Validation Loss: 2.7833, Validation Accuracy: 0.2576\n",
            "Epoch 3/25, Training Loss: 2.7454, Training Accuracy: 0.2653, Validation Loss: 2.7302, Validation Accuracy: 0.2650\n",
            "Epoch 4/25, Training Loss: 2.7509, Training Accuracy: 0.2654, Validation Loss: 2.7870, Validation Accuracy: 0.2527\n",
            "Epoch 5/25, Training Loss: 2.7609, Training Accuracy: 0.2631, Validation Loss: 2.7405, Validation Accuracy: 0.2801\n",
            "Epoch 6/25, Training Loss: 2.7681, Training Accuracy: 0.2631, Validation Loss: 2.7801, Validation Accuracy: 0.2547\n",
            "Epoch 7/25, Training Loss: 2.7728, Training Accuracy: 0.2576, Validation Loss: 2.7305, Validation Accuracy: 0.2574\n",
            "Epoch 8/25, Training Loss: 2.7491, Training Accuracy: 0.2650, Validation Loss: 2.7240, Validation Accuracy: 0.2545\n",
            "Epoch 9/25, Training Loss: 2.7447, Training Accuracy: 0.2674, Validation Loss: 2.7325, Validation Accuracy: 0.2728\n",
            "Epoch 10/25, Training Loss: 2.7428, Training Accuracy: 0.2667, Validation Loss: 2.7156, Validation Accuracy: 0.2738\n",
            "Epoch 11/25, Training Loss: 2.7434, Training Accuracy: 0.2670, Validation Loss: 2.7531, Validation Accuracy: 0.2618\n",
            "Epoch 12/25, Training Loss: 2.7429, Training Accuracy: 0.2672, Validation Loss: 2.7271, Validation Accuracy: 0.2656\n",
            "Epoch 13/25, Training Loss: 2.7439, Training Accuracy: 0.2675, Validation Loss: 2.7147, Validation Accuracy: 0.2670\n",
            "Epoch 14/25, Training Loss: 2.7424, Training Accuracy: 0.2677, Validation Loss: 2.7674, Validation Accuracy: 0.2607\n",
            "Epoch 15/25, Training Loss: 2.7422, Training Accuracy: 0.2674, Validation Loss: 2.7148, Validation Accuracy: 0.2747\n",
            "Epoch 16/25, Training Loss: 2.7440, Training Accuracy: 0.2674, Validation Loss: 2.7542, Validation Accuracy: 0.2614\n",
            "Epoch 17/25, Training Loss: 2.7431, Training Accuracy: 0.2680, Validation Loss: 2.7314, Validation Accuracy: 0.2619\n",
            "Epoch 18/25, Training Loss: 2.7424, Training Accuracy: 0.2675, Validation Loss: 2.7315, Validation Accuracy: 0.2649\n",
            "Epoch 19/25, Training Loss: 2.7424, Training Accuracy: 0.2674, Validation Loss: 2.7623, Validation Accuracy: 0.2801\n",
            "Epoch 20/25, Training Loss: 2.7433, Training Accuracy: 0.2675, Validation Loss: 2.7302, Validation Accuracy: 0.2634\n",
            "Epoch 21/25, Training Loss: 2.7439, Training Accuracy: 0.2671, Validation Loss: 2.7339, Validation Accuracy: 0.2478\n",
            "Epoch 22/25, Training Loss: 2.7433, Training Accuracy: 0.2671, Validation Loss: 2.7446, Validation Accuracy: 0.2547\n",
            "Epoch 23/25, Training Loss: 2.7423, Training Accuracy: 0.2671, Validation Loss: 2.7348, Validation Accuracy: 0.2545\n",
            "Epoch 24/25, Training Loss: 2.7437, Training Accuracy: 0.2668, Validation Loss: 2.7151, Validation Accuracy: 0.2569\n",
            "Epoch 25/25, Training Loss: 2.7421, Training Accuracy: 0.2678, Validation Loss: 2.6905, Validation Accuracy: 0.2757\n",
            "Training time: 3592.8662734031677 seconds\n"
          ]
        }
      ],
      "source": [
        "# Training the model\n",
        "start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_X)\n",
        "        loss = criterion(output, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += batch_y.size(0)\n",
        "        correct += (predicted == batch_y).sum().item()\n",
        "    train_accuracy = correct / total\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch_X, batch_y in val_loader:\n",
        "            val_output = model(batch_X)\n",
        "            val_loss = criterion(val_output, batch_y)\n",
        "            total_val_loss += val_loss.item()\n",
        "            _, predicted = torch.max(val_output.data, 1)\n",
        "            total += batch_y.size(0)\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "    val_accuracy = correct / total\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    \"\"\"\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "    \"\"\"\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(f\"Training time: {training_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tb1RaHkUa70",
        "outputId": "f19ed0e5-2b9b-4d94-94b3-a0f37787b848"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FIRST TEST STRING: This is a simple example to demonstrate how to predict the next char..\n",
            "First predicted next character: 'e'\n",
            "\n",
            "SECOND TEST STRING: Long live the quee..\n",
            "Second predicted next character: ' '\n",
            "\n",
            "THIRD TEST STRING: Be quiet and do not spea..\n",
            "Third predicted next character: 't'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Prediction function\n",
        "def predict_next_char(model, char_to_int, int_to_char, initial_str):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        initial_input = torch.tensor([char_to_int[c] for c in initial_str[-sequence_length:]], dtype=torch.long).unsqueeze(0)\n",
        "        prediction = model(initial_input)\n",
        "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
        "        return int_to_char[predicted_index]\n",
        "\n",
        "# Predicting the first next character\n",
        "test_str = \"This is a simple example to demonstrate how to predict the next char\"\n",
        "predicted_char = predict_next_char(model, char_to_int, int_to_char, test_str)\n",
        "print('FIRST TEST STRING: This is a simple example to demonstrate how to predict the next char..')\n",
        "print(f\"First predicted next character: '{predicted_char}'\")\n",
        "print(\"\")\n",
        "\n",
        "# Predicting the second next character\n",
        "test_str = \"Long live the quee\"\n",
        "predicted_char = predict_next_char(model, char_to_int, int_to_char, test_str)\n",
        "print('SECOND TEST STRING: Long live the quee..')\n",
        "print(f\"Second predicted next character: '{predicted_char}'\")\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# Predicting the third next character\n",
        "test_str = \"Be quiet and do not spea\"\n",
        "predicted_char = predict_next_char(model, char_to_int, int_to_char, test_str)\n",
        "print('THIRD TEST STRING: Be quiet and do not spea..')\n",
        "print(f\"Third predicted next character: '{predicted_char}'\")\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uO-f-zXsWC-"
      },
      "source": [
        "**MAXIMUM LENGTH OF INPUT SECQUENCES = 50**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vT-Fx3EDscAz"
      },
      "outputs": [],
      "source": [
        "#Prepare the dataset\n",
        "sequence_length = 50\n",
        "# Create a character mapping to integers\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
        "int_to_char = {i: ch for i, ch in enumerate(chars)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7RnXSn6tgJ2"
      },
      "outputs": [],
      "source": [
        "# Encode the text into integers\n",
        "encoded_text = [char_to_int[ch] for ch in text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UpehoKotivU"
      },
      "outputs": [],
      "source": [
        "# Create sequences and targets\n",
        "sequences = []\n",
        "targets = []\n",
        "for i in range(0, len(encoded_text) - sequence_length):\n",
        "    seq = encoded_text[i:i+sequence_length]\n",
        "    target = encoded_text[i+sequence_length]\n",
        "    sequences.append(seq)\n",
        "    targets.append(target)\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "targets = np.array(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uC4fSsNWhavB"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(sequences, targets, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWkNO_tJhbg5"
      },
      "outputs": [],
      "source": [
        "# Converting data to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.long)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val = torch.tensor(X_val, dtype=torch.long)\n",
        "y_val = torch.tensor(y_val, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Plh3CGythgFH"
      },
      "outputs": [],
      "source": [
        "#Create a dataset class\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.sequences = sequences\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.sequences[index], self.targets[index]\n",
        "\n",
        "# Instantiate the dataset\n",
        "dataset = CharDataset(sequences, targets)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets and data loaders\n",
        "batch_size = 64\n",
        "train_dataset = CharDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = CharDataset(X_val, y_val)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "M0K-d8mdSOpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_J-3wq4tw3Q"
      },
      "outputs": [],
      "source": [
        "# Defining the RNN model\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        #This line takes the input tensor x, which contains indices of characters, and passes it through an embedding layer (self.embedding).\n",
        "        #The embedding layer converts these indices into dense vectors of fixed size.\n",
        "        #These vectors are learned during training and can capture semantic similarities between characters.\n",
        "        #The result is a higher-dimensional representation of the input sequence, where each character index is replaced by its corresponding embedding vector.\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.rnn = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        #The RNN layer returns two outputs:\n",
        "        #1- the output tensor containing the output of the RNN at each time step for each sequence in the batch,\n",
        "        #2-the hidden state (_) of the last time step (which is not used in this line, hence the underscore).\n",
        "        output, _ = self.rnn(embedded)\n",
        "        #The RNN's output contains the outputs for every time step,\n",
        "        #but for this task, we're only interested in the output of the last time step because we're predicting the next character after the sequence.\n",
        "        #output[:, -1, :] selects the last time step's output for every sequence in the batch (-1 indexes the last item in Python).\n",
        "        output = self.fc(output[:, -1, :])  # Get the output of the last RNN cell\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKBQ7FyatxpW"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "hidden_size = 50\n",
        "learning_rate = 0.01\n",
        "epochs = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0thu2bGnt0tA"
      },
      "outputs": [],
      "source": [
        "# Model, loss, and optimizer\n",
        "model = CharRNN(len(chars), hidden_size, len(chars)+20) # Increase the number of output neurons by 20\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3chkidUDqHCk",
        "outputId": "94f891c1-147c-48f5-f949-09daad51b497"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27985"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "#count trainable parameters of the model\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZVR82dQt5CS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b36b96b3-9dab-4c6d-f387-ae55c901a6dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25, Training Loss: 1.9491, Training Accuracy: 0.4311, Validation Loss: 1.8681, Validation Accuracy: 0.4487\n",
            "Epoch 2/25, Training Loss: 1.8512, Training Accuracy: 0.4552, Validation Loss: 1.8476, Validation Accuracy: 0.4565\n",
            "Epoch 3/25, Training Loss: 1.8454, Training Accuracy: 0.4567, Validation Loss: 1.8576, Validation Accuracy: 0.4562\n",
            "Epoch 4/25, Training Loss: 1.8538, Training Accuracy: 0.4544, Validation Loss: 1.8787, Validation Accuracy: 0.4457\n",
            "Epoch 5/25, Training Loss: 1.8625, Training Accuracy: 0.4522, Validation Loss: 1.8663, Validation Accuracy: 0.4534\n",
            "Epoch 6/25, Training Loss: 1.8798, Training Accuracy: 0.4485, Validation Loss: 1.8893, Validation Accuracy: 0.4416\n",
            "Epoch 7/25, Training Loss: 1.8860, Training Accuracy: 0.4474, Validation Loss: 1.8894, Validation Accuracy: 0.4442\n",
            "Epoch 8/25, Training Loss: 1.8932, Training Accuracy: 0.4452, Validation Loss: 1.8957, Validation Accuracy: 0.4450\n",
            "Epoch 9/25, Training Loss: 1.9036, Training Accuracy: 0.4429, Validation Loss: 1.9114, Validation Accuracy: 0.4448\n",
            "Epoch 10/25, Training Loss: 1.9135, Training Accuracy: 0.4409, Validation Loss: 1.9346, Validation Accuracy: 0.4354\n",
            "Epoch 11/25, Training Loss: 1.9266, Training Accuracy: 0.4372, Validation Loss: 1.9250, Validation Accuracy: 0.4385\n",
            "Epoch 12/25, Training Loss: 1.9302, Training Accuracy: 0.4371, Validation Loss: 1.9366, Validation Accuracy: 0.4336\n",
            "Epoch 13/25, Training Loss: 1.9264, Training Accuracy: 0.4381, Validation Loss: 1.9443, Validation Accuracy: 0.4342\n",
            "Epoch 14/25, Training Loss: 1.9361, Training Accuracy: 0.4355, Validation Loss: 1.9516, Validation Accuracy: 0.4335\n",
            "Epoch 15/25, Training Loss: 1.9440, Training Accuracy: 0.4330, Validation Loss: 1.9506, Validation Accuracy: 0.4344\n",
            "Epoch 16/25, Training Loss: 1.9452, Training Accuracy: 0.4325, Validation Loss: 1.9554, Validation Accuracy: 0.4324\n",
            "Epoch 17/25, Training Loss: 1.9529, Training Accuracy: 0.4310, Validation Loss: 1.9678, Validation Accuracy: 0.4231\n",
            "Epoch 18/25, Training Loss: 1.9494, Training Accuracy: 0.4319, Validation Loss: 1.9688, Validation Accuracy: 0.4301\n",
            "Epoch 19/25, Training Loss: 1.9643, Training Accuracy: 0.4283, Validation Loss: 1.9755, Validation Accuracy: 0.4243\n",
            "Epoch 20/25, Training Loss: 1.9638, Training Accuracy: 0.4279, Validation Loss: 1.9759, Validation Accuracy: 0.4267\n",
            "Epoch 21/25, Training Loss: 1.9797, Training Accuracy: 0.4256, Validation Loss: 1.9868, Validation Accuracy: 0.4278\n",
            "Epoch 22/25, Training Loss: 1.9860, Training Accuracy: 0.4254, Validation Loss: 1.9849, Validation Accuracy: 0.4212\n",
            "Epoch 23/25, Training Loss: 1.9828, Training Accuracy: 0.4243, Validation Loss: 1.9905, Validation Accuracy: 0.4239\n",
            "Epoch 24/25, Training Loss: 1.9877, Training Accuracy: 0.4223, Validation Loss: 1.9910, Validation Accuracy: 0.4217\n",
            "Epoch 25/25, Training Loss: 2.0034, Training Accuracy: 0.4196, Validation Loss: 2.0249, Validation Accuracy: 0.4157\n",
            "Training time: 15817.772554636002 seconds\n"
          ]
        }
      ],
      "source": [
        "# Training the model\n",
        "start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_X)\n",
        "        loss = criterion(output, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += batch_y.size(0)\n",
        "        correct += (predicted == batch_y).sum().item()\n",
        "    train_accuracy = correct / total\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch_X, batch_y in val_loader:\n",
        "            val_output = model(batch_X)\n",
        "            val_loss = criterion(val_output, batch_y)\n",
        "            total_val_loss += val_loss.item()\n",
        "            _, predicted = torch.max(val_output.data, 1)\n",
        "            total += batch_y.size(0)\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "    val_accuracy = correct / total\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    \"\"\"\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "    \"\"\"\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(f\"Training time: {training_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TBIVAxrt77O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea5cc54c-378b-4ce8-beb5-9d9ca1b68d9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FIRST TEST STRING: This is a simple example to demonstrate how to predict the next char..\n",
            "First predicted next character: 'd'\n",
            "\n",
            "SECOND TEST STRING: Long live the quee..\n",
            "Second predicted next character: 'n'\n",
            "\n",
            "THIRD TEST STRING: Be quiet and do not spea..\n",
            "Third predicted next character: 'k'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Prediction function\n",
        "def predict_next_char(model, char_to_int, int_to_char, initial_str):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        initial_input = torch.tensor([char_to_int[c] for c in initial_str[-sequence_length:]], dtype=torch.long).unsqueeze(0)\n",
        "        prediction = model(initial_input)\n",
        "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
        "        return int_to_char[predicted_index]\n",
        "\n",
        "# Predicting the first next character\n",
        "test_str = \"This is a simple example to demonstrate how to predict the next char\"\n",
        "predicted_char = predict_next_char(model, char_to_int, int_to_char, test_str)\n",
        "print('FIRST TEST STRING: This is a simple example to demonstrate how to predict the next char..')\n",
        "print(f\"First predicted next character: '{predicted_char}'\")\n",
        "print(\"\")\n",
        "\n",
        "# Predicting the second next character\n",
        "test_str = \"Long live the quee\"\n",
        "predicted_char = predict_next_char(model, char_to_int, int_to_char, test_str)\n",
        "print('SECOND TEST STRING: Long live the quee..')\n",
        "print(f\"Second predicted next character: '{predicted_char}'\")\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# Predicting the third next character\n",
        "test_str = \"Be quiet and do not spea\"\n",
        "predicted_char = predict_next_char(model, char_to_int, int_to_char, test_str)\n",
        "print('THIRD TEST STRING: Be quiet and do not spea..')\n",
        "print(f\"Third predicted next character: '{predicted_char}'\")\n",
        "print(\"\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}