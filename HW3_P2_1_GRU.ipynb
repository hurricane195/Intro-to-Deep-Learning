{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hurricane195/Intro-to-Deep-Learning/blob/Homework_3/HW3_P2_1_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXJ7QOmqv1p9"
      },
      "source": [
        "**Build the model for.LSTM** and rnn.GRU **for the tiny Shakespeare dataset, the data loader code is already provided.**\n",
        "\n",
        "**Train the models for the sequence of 20 and 30, report and compare training loss, validation accuracy, execution time for training, and computational and mode size complexities across the two models.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MC4eRxw1vpcG"
      },
      "outputs": [],
      "source": [
        "#Using a modided example of Dr. Tabkhi's \"RNN\" available at https://github.com/HamedTabkhi/Intro-to-DL/blob/main/RNN.py\n",
        "#Using a modided example of Dr. Tabkhi's \"RNN-CharDataset\" available at https://github.com/HamedTabkhi/Intro-to-DL/blob/main/RNN-CharDataset.py\n",
        "#Using a modided example of Dr. Tabkhi's \"shakespeare-loader.py\" available at https://github.com/HamedTabkhi/Intro-to-DL/blob/main/shakespeare-loader.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsIqPri1TA1l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "199KcmHMx1vo",
        "outputId": "faa0df51-42d7-4fdf-d144-4fc1fd539c51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check for CUDA support and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E-V7pjorX53"
      },
      "outputs": [],
      "source": [
        "#Download the dataset\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text  # This is the entire text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAxQQGCMsM4t"
      },
      "source": [
        "**MAXIMUM LENGTH OF INPUT SECQUENCES = 20**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ty3Ss4JVrdV_"
      },
      "outputs": [],
      "source": [
        "#Prepare the dataset\n",
        "sequence_length = 20\n",
        "# Create a character mapping to integers\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
        "int_to_char = {i: ch for i, ch in enumerate(chars)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIuvc0zPrhzj"
      },
      "outputs": [],
      "source": [
        "# Encode the text into integers\n",
        "encoded_text = [char_to_int[ch] for ch in text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4EWMpZRrkcR"
      },
      "outputs": [],
      "source": [
        "# Create sequences and targets\n",
        "sequences = []\n",
        "targets = []\n",
        "for i in range(0, len(encoded_text) - sequence_length):\n",
        "    seq = encoded_text[i:i+sequence_length]\n",
        "    target = encoded_text[i+sequence_length]\n",
        "    sequences.append(seq)\n",
        "    targets.append(target)\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "targets = np.array(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-updB7piHC6"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(sequences, targets, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofrCzO7QilqM"
      },
      "outputs": [],
      "source": [
        "# Converting data to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.long)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val = torch.tensor(X_val, dtype=torch.long)\n",
        "y_val = torch.tensor(y_val, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot47W5O6rpVt"
      },
      "outputs": [],
      "source": [
        "#Create a dataset class\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.sequences = sequences\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.sequences[index], self.targets[index]\n",
        "\n",
        "# Instantiate the dataset\n",
        "dataset = CharDataset(sequences, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaYFkmSkrwPN"
      },
      "outputs": [],
      "source": [
        "# Create datasets and data loaders\n",
        "batch_size = 128\n",
        "train_dataset = CharDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = CharDataset(X_val, y_val)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37fojvqSTe_7"
      },
      "outputs": [],
      "source": [
        "# Defining the RNN model\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        #This line takes the input tensor x, which contains indices of characters, and passes it through an embedding layer (self.embedding).\n",
        "        #The embedding layer converts these indices into dense vectors of fixed size.\n",
        "        #These vectors are learned during training and can capture semantic similarities between characters.\n",
        "        #The result is a higher-dimensional representation of the input sequence, where each character index is replaced by its corresponding embedding vector.\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        #The RNN layer returns two outputs:\n",
        "        #1- the output tensor containing the output of the RNN at each time step for each sequence in the batch,\n",
        "        #2-the hidden state (_) of the last time step (which is not used in this line, hence the underscore).\n",
        "        output, _ = self.rnn(embedded)\n",
        "        #The RNN's output contains the outputs for every time step,\n",
        "        #but for this task, we're only interested in the output of the last time step because we're predicting the next character after the sequence.\n",
        "        #output[:, -1, :] selects the last time step's output for every sequence in the batch (-1 indexes the last item in Python).\n",
        "        output = self.fc(output[:, -1, :])  # Get the output of the last RNN cell\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td4AGgC2TiHl"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "hidden_size = 100\n",
        "learning_rate = 0.001\n",
        "epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08mM2r-QTkFT"
      },
      "outputs": [],
      "source": [
        "# Model, loss, and optimizer\n",
        "model = CharRNN(len(chars), hidden_size, len(chars))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3IEg0wRqCC1",
        "outputId": "82644ec6-9d9d-4e99-c425-b4ab09c8b05e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "73665"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#count trainable parameters of the model\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqpBZgkwUXuf",
        "outputId": "aade939d-30b3-4e8e-d62a-53b7ed25a0ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50, Training Loss: 1.8550, Training Accuracy: 0.4588, Validation Loss: 1.6692, Validation Accuracy: 0.5046\n",
            "Epoch 2/50, Training Loss: 1.6146, Training Accuracy: 0.5171, Validation Loss: 1.5878, Validation Accuracy: 0.5242\n",
            "Epoch 3/50, Training Loss: 1.5582, Training Accuracy: 0.5312, Validation Loss: 1.5559, Validation Accuracy: 0.5331\n",
            "Epoch 4/50, Training Loss: 1.5286, Training Accuracy: 0.5395, Validation Loss: 1.5310, Validation Accuracy: 0.5384\n",
            "Epoch 5/50, Training Loss: 1.5090, Training Accuracy: 0.5441, Validation Loss: 1.5172, Validation Accuracy: 0.5415\n",
            "Epoch 6/50, Training Loss: 1.4955, Training Accuracy: 0.5477, Validation Loss: 1.5110, Validation Accuracy: 0.5439\n",
            "Epoch 7/50, Training Loss: 1.4859, Training Accuracy: 0.5495, Validation Loss: 1.5039, Validation Accuracy: 0.5466\n",
            "Epoch 8/50, Training Loss: 1.4770, Training Accuracy: 0.5516, Validation Loss: 1.4966, Validation Accuracy: 0.5475\n",
            "Epoch 9/50, Training Loss: 1.4714, Training Accuracy: 0.5532, Validation Loss: 1.4904, Validation Accuracy: 0.5488\n",
            "Epoch 10/50, Training Loss: 1.4655, Training Accuracy: 0.5548, Validation Loss: 1.4905, Validation Accuracy: 0.5477\n",
            "Epoch 11/50, Training Loss: 1.4616, Training Accuracy: 0.5553, Validation Loss: 1.4888, Validation Accuracy: 0.5495\n",
            "Epoch 12/50, Training Loss: 1.4575, Training Accuracy: 0.5566, Validation Loss: 1.4868, Validation Accuracy: 0.5501\n",
            "Epoch 13/50, Training Loss: 1.4544, Training Accuracy: 0.5573, Validation Loss: 1.4829, Validation Accuracy: 0.5512\n",
            "Epoch 14/50, Training Loss: 1.4514, Training Accuracy: 0.5579, Validation Loss: 1.4799, Validation Accuracy: 0.5517\n",
            "Epoch 15/50, Training Loss: 1.4491, Training Accuracy: 0.5586, Validation Loss: 1.4750, Validation Accuracy: 0.5536\n",
            "Epoch 16/50, Training Loss: 1.4468, Training Accuracy: 0.5590, Validation Loss: 1.4774, Validation Accuracy: 0.5526\n",
            "Epoch 17/50, Training Loss: 1.4447, Training Accuracy: 0.5593, Validation Loss: 1.4754, Validation Accuracy: 0.5524\n",
            "Epoch 18/50, Training Loss: 1.4428, Training Accuracy: 0.5600, Validation Loss: 1.4739, Validation Accuracy: 0.5534\n",
            "Epoch 19/50, Training Loss: 1.4416, Training Accuracy: 0.5606, Validation Loss: 1.4738, Validation Accuracy: 0.5530\n",
            "Epoch 20/50, Training Loss: 1.4395, Training Accuracy: 0.5605, Validation Loss: 1.4729, Validation Accuracy: 0.5542\n",
            "Epoch 21/50, Training Loss: 1.4387, Training Accuracy: 0.5607, Validation Loss: 1.4738, Validation Accuracy: 0.5531\n",
            "Epoch 22/50, Training Loss: 1.4374, Training Accuracy: 0.5616, Validation Loss: 1.4800, Validation Accuracy: 0.5519\n",
            "Epoch 23/50, Training Loss: 1.4363, Training Accuracy: 0.5615, Validation Loss: 1.4713, Validation Accuracy: 0.5549\n",
            "Epoch 24/50, Training Loss: 1.4353, Training Accuracy: 0.5622, Validation Loss: 1.4671, Validation Accuracy: 0.5556\n",
            "Epoch 25/50, Training Loss: 1.4342, Training Accuracy: 0.5620, Validation Loss: 1.4699, Validation Accuracy: 0.5540\n",
            "Epoch 26/50, Training Loss: 1.4338, Training Accuracy: 0.5621, Validation Loss: 1.4733, Validation Accuracy: 0.5538\n",
            "Epoch 27/50, Training Loss: 1.4333, Training Accuracy: 0.5620, Validation Loss: 1.4676, Validation Accuracy: 0.5562\n",
            "Epoch 28/50, Training Loss: 1.4327, Training Accuracy: 0.5624, Validation Loss: 1.4687, Validation Accuracy: 0.5552\n",
            "Epoch 29/50, Training Loss: 1.4320, Training Accuracy: 0.5624, Validation Loss: 1.4663, Validation Accuracy: 0.5567\n",
            "Epoch 30/50, Training Loss: 1.4311, Training Accuracy: 0.5628, Validation Loss: 1.4678, Validation Accuracy: 0.5557\n",
            "Epoch 31/50, Training Loss: 1.4306, Training Accuracy: 0.5627, Validation Loss: 1.4652, Validation Accuracy: 0.5550\n",
            "Epoch 32/50, Training Loss: 1.4305, Training Accuracy: 0.5631, Validation Loss: 1.4642, Validation Accuracy: 0.5569\n",
            "Epoch 33/50, Training Loss: 1.4297, Training Accuracy: 0.5634, Validation Loss: 1.4684, Validation Accuracy: 0.5539\n",
            "Epoch 34/50, Training Loss: 1.4298, Training Accuracy: 0.5634, Validation Loss: 1.4643, Validation Accuracy: 0.5555\n",
            "Epoch 35/50, Training Loss: 1.4290, Training Accuracy: 0.5630, Validation Loss: 1.4678, Validation Accuracy: 0.5553\n",
            "Epoch 36/50, Training Loss: 1.4286, Training Accuracy: 0.5633, Validation Loss: 1.4660, Validation Accuracy: 0.5560\n",
            "Epoch 37/50, Training Loss: 1.4291, Training Accuracy: 0.5631, Validation Loss: 1.4686, Validation Accuracy: 0.5547\n",
            "Epoch 38/50, Training Loss: 1.4296, Training Accuracy: 0.5632, Validation Loss: 1.4652, Validation Accuracy: 0.5565\n",
            "Epoch 39/50, Training Loss: 1.4294, Training Accuracy: 0.5632, Validation Loss: 1.4693, Validation Accuracy: 0.5554\n",
            "Epoch 40/50, Training Loss: 1.4294, Training Accuracy: 0.5636, Validation Loss: 1.4675, Validation Accuracy: 0.5558\n",
            "Epoch 41/50, Training Loss: 1.4293, Training Accuracy: 0.5632, Validation Loss: 1.4679, Validation Accuracy: 0.5552\n",
            "Epoch 42/50, Training Loss: 1.4300, Training Accuracy: 0.5628, Validation Loss: 1.4674, Validation Accuracy: 0.5555\n",
            "Epoch 43/50, Training Loss: 1.4286, Training Accuracy: 0.5636, Validation Loss: 1.4663, Validation Accuracy: 0.5560\n",
            "Epoch 44/50, Training Loss: 1.4298, Training Accuracy: 0.5629, Validation Loss: 1.4644, Validation Accuracy: 0.5571\n",
            "Epoch 45/50, Training Loss: 1.4294, Training Accuracy: 0.5633, Validation Loss: 1.4681, Validation Accuracy: 0.5566\n",
            "Epoch 46/50, Training Loss: 1.4289, Training Accuracy: 0.5630, Validation Loss: 1.4618, Validation Accuracy: 0.5566\n",
            "Epoch 47/50, Training Loss: 1.4303, Training Accuracy: 0.5632, Validation Loss: 1.4656, Validation Accuracy: 0.5579\n",
            "Epoch 48/50, Training Loss: 1.4295, Training Accuracy: 0.5628, Validation Loss: 1.4644, Validation Accuracy: 0.5573\n",
            "Epoch 49/50, Training Loss: 1.4291, Training Accuracy: 0.5629, Validation Loss: 1.4680, Validation Accuracy: 0.5555\n",
            "Epoch 50/50, Training Loss: 1.4284, Training Accuracy: 0.5636, Validation Loss: 1.4659, Validation Accuracy: 0.5575\n",
            "Training time: 7407.700059175491 seconds\n"
          ]
        }
      ],
      "source": [
        "# Training the model\n",
        "start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_X)\n",
        "        loss = criterion(output, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += batch_y.size(0)\n",
        "        correct += (predicted == batch_y).sum().item()\n",
        "    train_accuracy = correct / total\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch_X, batch_y in val_loader:\n",
        "            val_output = model(batch_X)\n",
        "            val_loss = criterion(val_output, batch_y)\n",
        "            total_val_loss += val_loss.item()\n",
        "            _, predicted = torch.max(val_output.data, 1)\n",
        "            total += batch_y.size(0)\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "    val_accuracy = correct / total\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    \"\"\"\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "    \"\"\"\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(f\"Training time: {training_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1tb1RaHkUa70",
        "outputId": "9e87e678-c311-4a59-a695-8e3f16d810ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FIRST TEST STRING: This is a simple example to demonstrate how to predict the next char..\n",
            "First predicted next character: 'g'\n",
            "\n",
            "SECOND TEST STRING: Long live the quee..\n",
            "Second predicted next character: 'n'\n",
            "\n",
            "THIRD TEST STRING: Be quiet and do not spea..\n",
            "Third predicted next character: 'k'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Prediction function\n",
        "def predict_next_char(model, char_to_int, int_to_char, initial_str):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        initial_input = torch.tensor([char_to_int[c] for c in initial_str[-sequence_length:]], dtype=torch.long).unsqueeze(0)\n",
        "        prediction = model(initial_input)\n",
        "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
        "        return int_to_char[predicted_index]\n",
        "\n",
        "# Predicting the first next character\n",
        "test_str = \"This is a simple example to demonstrate how to predict the next char\"\n",
        "predicted_char = predict_next_char(model, char_to_int, int_to_char, test_str)\n",
        "print('FIRST TEST STRING: This is a simple example to demonstrate how to predict the next char..')\n",
        "print(f\"First predicted next character: '{predicted_char}'\")\n",
        "print(\"\")\n",
        "\n",
        "# Predicting the second next character\n",
        "test_str = \"Long live the quee\"\n",
        "predicted_char = predict_next_char(model, char_to_int, int_to_char, test_str)\n",
        "print('SECOND TEST STRING: Long live the quee..')\n",
        "print(f\"Second predicted next character: '{predicted_char}'\")\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# Predicting the third next character\n",
        "test_str = \"Be quiet and do not spea\"\n",
        "predicted_char = predict_next_char(model, char_to_int, int_to_char, test_str)\n",
        "print('THIRD TEST STRING: Be quiet and do not spea..')\n",
        "print(f\"Third predicted next character: '{predicted_char}'\")\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uO-f-zXsWC-"
      },
      "source": [
        "**MAXIMUM LENGTH OF INPUT SECQUENCES = 30**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vT-Fx3EDscAz"
      },
      "outputs": [],
      "source": [
        "#Prepare the dataset\n",
        "sequence_length = 30\n",
        "# Create a character mapping to integers\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
        "int_to_char = {i: ch for i, ch in enumerate(chars)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7RnXSn6tgJ2"
      },
      "outputs": [],
      "source": [
        "# Encode the text into integers\n",
        "encoded_text = [char_to_int[ch] for ch in text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UpehoKotivU"
      },
      "outputs": [],
      "source": [
        "# Create sequences and targets\n",
        "sequences = []\n",
        "targets = []\n",
        "for i in range(0, len(encoded_text) - sequence_length):\n",
        "    seq = encoded_text[i:i+sequence_length]\n",
        "    target = encoded_text[i+sequence_length]\n",
        "    sequences.append(seq)\n",
        "    targets.append(target)\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "targets = np.array(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uC4fSsNWhavB"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(sequences, targets, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWkNO_tJhbg5"
      },
      "outputs": [],
      "source": [
        "# Converting data to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.long)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val = torch.tensor(X_val, dtype=torch.long)\n",
        "y_val = torch.tensor(y_val, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Plh3CGythgFH"
      },
      "outputs": [],
      "source": [
        "#Create a dataset class\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.sequences = sequences\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.sequences[index], self.targets[index]\n",
        "\n",
        "# Instantiate the dataset\n",
        "dataset = CharDataset(sequences, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0K-d8mdSOpK"
      },
      "outputs": [],
      "source": [
        "# Create datasets and data loaders\n",
        "batch_size = 128\n",
        "train_dataset = CharDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = CharDataset(X_val, y_val)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_J-3wq4tw3Q"
      },
      "outputs": [],
      "source": [
        "# Defining the RNN model\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        #This line takes the input tensor x, which contains indices of characters, and passes it through an embedding layer (self.embedding).\n",
        "        #The embedding layer converts these indices into dense vectors of fixed size.\n",
        "        #These vectors are learned during training and can capture semantic similarities between characters.\n",
        "        #The result is a higher-dimensional representation of the input sequence, where each character index is replaced by its corresponding embedding vector.\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        #The RNN layer returns two outputs:\n",
        "        #1- the output tensor containing the output of the RNN at each time step for each sequence in the batch,\n",
        "        #2-the hidden state (_) of the last time step (which is not used in this line, hence the underscore).\n",
        "        output, _ = self.rnn(embedded)\n",
        "        #The RNN's output contains the outputs for every time step,\n",
        "        #but for this task, we're only interested in the output of the last time step because we're predicting the next character after the sequence.\n",
        "        #output[:, -1, :] selects the last time step's output for every sequence in the batch (-1 indexes the last item in Python).\n",
        "        output = self.fc(output[:, -1, :])  # Get the output of the last RNN cell\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKBQ7FyatxpW"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "hidden_size = 100\n",
        "learning_rate = 0.001\n",
        "epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0thu2bGnt0tA"
      },
      "outputs": [],
      "source": [
        "# Model, loss, and optimizer\n",
        "model = CharRNN(len(chars), hidden_size, len(chars))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3chkidUDqHCk",
        "outputId": "2c96a2bc-dd4d-4239-c69b-1fb6fa4c4a0d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "73665"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "#count trainable parameters of the model\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZVR82dQt5CS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f413e2-0bf1-44fc-efc4-3b7baac65a31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Training Loss: 1.8511, Training Accuracy: 0.4594, Validation Loss: 1.6671, Validation Accuracy: 0.5055\n",
            "Epoch 2/50, Training Loss: 1.6099, Training Accuracy: 0.5197, Validation Loss: 1.5884, Validation Accuracy: 0.5250\n",
            "Epoch 3/50, Training Loss: 1.5547, Training Accuracy: 0.5333, Validation Loss: 1.5555, Validation Accuracy: 0.5342\n",
            "Epoch 4/50, Training Loss: 1.5258, Training Accuracy: 0.5404, Validation Loss: 1.5360, Validation Accuracy: 0.5394\n",
            "Epoch 5/50, Training Loss: 1.5066, Training Accuracy: 0.5448, Validation Loss: 1.5238, Validation Accuracy: 0.5398\n",
            "Epoch 6/50, Training Loss: 1.4929, Training Accuracy: 0.5486, Validation Loss: 1.5102, Validation Accuracy: 0.5451\n",
            "Epoch 7/50, Training Loss: 1.4826, Training Accuracy: 0.5512, Validation Loss: 1.4985, Validation Accuracy: 0.5499\n",
            "Epoch 8/50, Training Loss: 1.4744, Training Accuracy: 0.5529, Validation Loss: 1.4988, Validation Accuracy: 0.5480\n",
            "Epoch 9/50, Training Loss: 1.4680, Training Accuracy: 0.5547, Validation Loss: 1.4892, Validation Accuracy: 0.5513\n",
            "Epoch 10/50, Training Loss: 1.4624, Training Accuracy: 0.5555, Validation Loss: 1.4875, Validation Accuracy: 0.5520\n",
            "Epoch 11/50, Training Loss: 1.4575, Training Accuracy: 0.5571, Validation Loss: 1.4860, Validation Accuracy: 0.5512\n",
            "Epoch 12/50, Training Loss: 1.4538, Training Accuracy: 0.5580, Validation Loss: 1.4837, Validation Accuracy: 0.5517\n",
            "Epoch 13/50, Training Loss: 1.4507, Training Accuracy: 0.5585, Validation Loss: 1.4772, Validation Accuracy: 0.5542\n",
            "Epoch 14/50, Training Loss: 1.4479, Training Accuracy: 0.5596, Validation Loss: 1.4802, Validation Accuracy: 0.5526\n",
            "Epoch 15/50, Training Loss: 1.4450, Training Accuracy: 0.5598, Validation Loss: 1.4785, Validation Accuracy: 0.5529\n",
            "Epoch 16/50, Training Loss: 1.4426, Training Accuracy: 0.5604, Validation Loss: 1.4770, Validation Accuracy: 0.5527\n",
            "Epoch 17/50, Training Loss: 1.4401, Training Accuracy: 0.5610, Validation Loss: 1.4745, Validation Accuracy: 0.5538\n",
            "Epoch 18/50, Training Loss: 1.4383, Training Accuracy: 0.5615, Validation Loss: 1.4742, Validation Accuracy: 0.5538\n",
            "Epoch 19/50, Training Loss: 1.4371, Training Accuracy: 0.5620, Validation Loss: 1.4748, Validation Accuracy: 0.5540\n",
            "Epoch 20/50, Training Loss: 1.4356, Training Accuracy: 0.5621, Validation Loss: 1.4755, Validation Accuracy: 0.5545\n",
            "Epoch 21/50, Training Loss: 1.4345, Training Accuracy: 0.5624, Validation Loss: 1.4728, Validation Accuracy: 0.5549\n",
            "Epoch 22/50, Training Loss: 1.4331, Training Accuracy: 0.5631, Validation Loss: 1.4673, Validation Accuracy: 0.5565\n",
            "Epoch 23/50, Training Loss: 1.4325, Training Accuracy: 0.5627, Validation Loss: 1.4686, Validation Accuracy: 0.5568\n",
            "Epoch 24/50, Training Loss: 1.4313, Training Accuracy: 0.5631, Validation Loss: 1.4681, Validation Accuracy: 0.5557\n",
            "Epoch 25/50, Training Loss: 1.4302, Training Accuracy: 0.5635, Validation Loss: 1.4724, Validation Accuracy: 0.5552\n",
            "Epoch 26/50, Training Loss: 1.4299, Training Accuracy: 0.5636, Validation Loss: 1.4689, Validation Accuracy: 0.5560\n",
            "Epoch 27/50, Training Loss: 1.4293, Training Accuracy: 0.5636, Validation Loss: 1.4656, Validation Accuracy: 0.5561\n",
            "Epoch 28/50, Training Loss: 1.4287, Training Accuracy: 0.5641, Validation Loss: 1.4665, Validation Accuracy: 0.5581\n",
            "Epoch 29/50, Training Loss: 1.4279, Training Accuracy: 0.5638, Validation Loss: 1.4675, Validation Accuracy: 0.5560\n",
            "Epoch 30/50, Training Loss: 1.4272, Training Accuracy: 0.5642, Validation Loss: 1.4693, Validation Accuracy: 0.5561\n",
            "Epoch 31/50, Training Loss: 1.4262, Training Accuracy: 0.5646, Validation Loss: 1.4633, Validation Accuracy: 0.5571\n",
            "Epoch 32/50, Training Loss: 1.4257, Training Accuracy: 0.5650, Validation Loss: 1.4638, Validation Accuracy: 0.5565\n",
            "Epoch 33/50, Training Loss: 1.4253, Training Accuracy: 0.5646, Validation Loss: 1.4686, Validation Accuracy: 0.5568\n",
            "Epoch 34/50, Training Loss: 1.4256, Training Accuracy: 0.5647, Validation Loss: 1.4681, Validation Accuracy: 0.5550\n",
            "Epoch 35/50, Training Loss: 1.4238, Training Accuracy: 0.5648, Validation Loss: 1.4621, Validation Accuracy: 0.5585\n",
            "Epoch 36/50, Training Loss: 1.4242, Training Accuracy: 0.5647, Validation Loss: 1.4679, Validation Accuracy: 0.5563\n",
            "Epoch 37/50, Training Loss: 1.4228, Training Accuracy: 0.5652, Validation Loss: 1.4637, Validation Accuracy: 0.5576\n",
            "Epoch 38/50, Training Loss: 1.4233, Training Accuracy: 0.5651, Validation Loss: 1.4667, Validation Accuracy: 0.5574\n",
            "Epoch 39/50, Training Loss: 1.4234, Training Accuracy: 0.5650, Validation Loss: 1.4680, Validation Accuracy: 0.5568\n",
            "Epoch 40/50, Training Loss: 1.4238, Training Accuracy: 0.5653, Validation Loss: 1.4637, Validation Accuracy: 0.5579\n",
            "Epoch 41/50, Training Loss: 1.4231, Training Accuracy: 0.5652, Validation Loss: 1.4634, Validation Accuracy: 0.5589\n",
            "Epoch 42/50, Training Loss: 1.4236, Training Accuracy: 0.5651, Validation Loss: 1.4672, Validation Accuracy: 0.5571\n",
            "Epoch 43/50, Training Loss: 1.4228, Training Accuracy: 0.5652, Validation Loss: 1.4656, Validation Accuracy: 0.5569\n",
            "Epoch 44/50, Training Loss: 1.4231, Training Accuracy: 0.5648, Validation Loss: 1.4828, Validation Accuracy: 0.5514\n",
            "Epoch 45/50, Training Loss: 1.4237, Training Accuracy: 0.5649, Validation Loss: 1.4688, Validation Accuracy: 0.5572\n",
            "Epoch 46/50, Training Loss: 1.4226, Training Accuracy: 0.5656, Validation Loss: 1.4670, Validation Accuracy: 0.5556\n",
            "Epoch 47/50, Training Loss: 1.4238, Training Accuracy: 0.5647, Validation Loss: 1.4722, Validation Accuracy: 0.5552\n",
            "Epoch 48/50, Training Loss: 1.4228, Training Accuracy: 0.5651, Validation Loss: 1.4651, Validation Accuracy: 0.5573\n",
            "Epoch 49/50, Training Loss: 1.4234, Training Accuracy: 0.5647, Validation Loss: 1.4669, Validation Accuracy: 0.5566\n",
            "Epoch 50/50, Training Loss: 1.4232, Training Accuracy: 0.5653, Validation Loss: 1.4654, Validation Accuracy: 0.5558\n",
            "Training time: 11760.396078109741 seconds\n"
          ]
        }
      ],
      "source": [
        "# Training the model\n",
        "start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_X)\n",
        "        loss = criterion(output, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += batch_y.size(0)\n",
        "        correct += (predicted == batch_y).sum().item()\n",
        "    train_accuracy = correct / total\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch_X, batch_y in val_loader:\n",
        "            val_output = model(batch_X)\n",
        "            val_loss = criterion(val_output, batch_y)\n",
        "            total_val_loss += val_loss.item()\n",
        "            _, predicted = torch.max(val_output.data, 1)\n",
        "            total += batch_y.size(0)\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "    val_accuracy = correct / total\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    \"\"\"\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "    \"\"\"\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(f\"Training time: {training_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TBIVAxrt77O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ec67308-1bf6-4d74-ce4f-b59ffd9be4c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FIRST TEST STRING: This is a simple example to demonstrate how to predict the next char..\n",
            "First predicted next character: 'g'\n",
            "\n",
            "SECOND TEST STRING: Long live the quee..\n",
            "Second predicted next character: 'n'\n",
            "\n",
            "THIRD TEST STRING: Be quiet and do not spea..\n",
            "Third predicted next character: 'k'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Prediction function\n",
        "def predict_next_char(model, char_to_int, int_to_char, initial_str):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        initial_input = torch.tensor([char_to_int[c] for c in initial_str[-sequence_length:]], dtype=torch.long).unsqueeze(0)\n",
        "        prediction = model(initial_input)\n",
        "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
        "        return int_to_char[predicted_index]\n",
        "\n",
        "# Predicting the first next character\n",
        "test_str = \"This is a simple example to demonstrate how to predict the next char\"\n",
        "predicted_char = predict_next_char(model, char_to_int, int_to_char, test_str)\n",
        "print('FIRST TEST STRING: This is a simple example to demonstrate how to predict the next char..')\n",
        "print(f\"First predicted next character: '{predicted_char}'\")\n",
        "print(\"\")\n",
        "\n",
        "# Predicting the second next character\n",
        "test_str = \"Long live the quee\"\n",
        "predicted_char = predict_next_char(model, char_to_int, int_to_char, test_str)\n",
        "print('SECOND TEST STRING: Long live the quee..')\n",
        "print(f\"Second predicted next character: '{predicted_char}'\")\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# Predicting the third next character\n",
        "test_str = \"Be quiet and do not spea\"\n",
        "predicted_char = predict_next_char(model, char_to_int, int_to_char, test_str)\n",
        "print('THIRD TEST STRING: Be quiet and do not spea..')\n",
        "print(f\"Third predicted next character: '{predicted_char}'\")\n",
        "print(\"\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}